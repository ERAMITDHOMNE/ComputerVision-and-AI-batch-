{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Artificial Neural Network .ipynb",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3A7optD8mRT"
      },
      "source": [
        "\n",
        "*Hands-on of Big Data Analyst with TuV Certified Qualification*\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRbPEO1kbvJy"
      },
      "source": [
        "# Artificial Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFtS44oVo6i8"
      },
      "source": [
        "Artificial neural networks (ANN) or connectionist systems are computing systems that are inspired by, but not necessarily identical to, the biological neural networks that constitute animal brains. [*WIkipedia*](https://en.wikipedia.org/wiki/Artificial_neural_network).\n",
        "\n",
        "In this practice, we will keep working on the Telco Cutomer Churn Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM7uFmDzZYcI"
      },
      "source": [
        "# Import Library\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Jsb_MvbvJ1"
      },
      "source": [
        "#Import the files to Google Colab\n",
        "url = 'https://raw.githubusercontent.com/rc-dbe/bigdatacertification/master/dataset/churn_trasnsformed_new.csv'\n",
        "df_csv = pd.read_csv(url, sep=',',)\n",
        "\n",
        "# Show 10 first Row\n",
        "df_csv.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57jfIGX0bvJ8"
      },
      "source": [
        "# Remove \"Unnamed:O\" Coloumn\n",
        "df = df_csv.drop(\"Unnamed: 0\", axis=1)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Gvh7SNbvJ_"
      },
      "source": [
        "# Check the Data Infomation\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx45h455l68m"
      },
      "source": [
        "#Import MinMax Scaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# initialize min-max scaler\n",
        "mm_scaler = MinMaxScaler()\n",
        "column_names = df.columns.tolist()\n",
        "column_names.remove('Churn')\n",
        "\n",
        "# Transform all attributes\n",
        "df[column_names] = mm_scaler.fit_transform(df[column_names])\n",
        "df.sort_index(inplace=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9ZbOQ99bvKD"
      },
      "source": [
        "# Selecting the Feature, by remove the unused feature\n",
        "feature = ['Churn']\n",
        "train_feature = df.drop(feature, axis=1)\n",
        "\n",
        "# Set The Target\n",
        "train_target = df[\"Churn\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_pFrFx9bvKF"
      },
      "source": [
        "# Show the Feature\n",
        "train_feature.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpZEgMysbvKI"
      },
      "source": [
        "# Split Data\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_feature ,train_target, shuffle = True, test_size=0.3, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UPYs0JsbvKK"
      },
      "source": [
        "# Show the training data\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_noknfbmjiB"
      },
      "source": [
        "To train the ANN Model.We will use the MLPClassifier from Scikit Learn Library. The full documentation can be seen [HERE](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). Below is the default parameter:\n",
        "\n",
        "`sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100), activation=’relu’, solver=’adam’, alpha=0.0001, batch_size=’auto’, learning_rate=’constant’, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4on-FPo2bvKO"
      },
      "source": [
        "# Import Library\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Fitting Model\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(5), activation = 'relu', solver = 'adam',max_iter= 10000, verbose = True)\n",
        "mlp = mlp.fit(X_train,y_train)\n",
        "\n",
        "# Prediction to Test Dataset\n",
        "y_predmlp = mlp.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnY4gSKGSW2_"
      },
      "source": [
        "print('Number of Layer =', mlp.n_layers_)\n",
        "print('Number of Iteration =', mlp.n_iter_)\n",
        "print('Current loss computed with the loss function =', mlp.loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0RyWZq8qCH9"
      },
      "source": [
        "Since it was the classification problem, we can evaluate the model using Confussion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6KkGY21bvKR"
      },
      "source": [
        "# Import the metrics class\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "# Confussion Matrix\n",
        "cnf_matrixmlp = metrics.confusion_matrix(y_test, y_predmlp)\n",
        "cnf_matrixmlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di4T2Cf4bvKU"
      },
      "source": [
        "# Show the Accuracy, Precision, Recall, F1, etc.\n",
        "acc_mlp = metrics.accuracy_score(y_test, y_predmlp)\n",
        "prec_mlp = metrics.precision_score(y_test, y_predmlp)\n",
        "rec_mlp = metrics.recall_score(y_test, y_predmlp)\n",
        "f1_mlp = metrics.f1_score(y_test, y_predmlp)\n",
        "kappa_mlp = metrics.cohen_kappa_score(y_test, y_predmlp)\n",
        "\n",
        "print(\"Accuracy:\", acc_mlp)\n",
        "print(\"Precision:\", prec_mlp)\n",
        "print(\"Recall:\", rec_mlp)\n",
        "print(\"F1 Score:\", f1_mlp)\n",
        "print(\"Cohens Kappa Score:\", kappa_mlp)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}